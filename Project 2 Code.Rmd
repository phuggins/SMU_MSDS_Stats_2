---
title: "R Notebook"
output: html_notebook
editor_options: 
  chunk_output_type: console
---

# Please see the below code and comments for Applied Stats Project 2, Dr. Turner, SMU
# Authors: Antonio Debouse, Paul Huggins, Lijju Matthew
# Thank you!
  
## Load libraries
```{r}
# Below are the required libraries for the project
library(GGally)
library(scales)
library(formattable)
library(leaps)
library(car)
library(lme4)
library(mlbench)
library(caret)
library(caretEnsemble)
library(tidyr)
library(MASS)
library(corrr)
library(randomForest)
library(taRifx)
library(ROSE)
library(arm)
library(glmnet)
library(imputeMissings)
library(ggvis)
library(mice)
library(ISLR)
library(plyr)
library(dplyr)
library(ISOweek)
library(corrplot)
library(PerformanceAnalytics)
library(psych)
library(psychTools)
library(mda)
library(klaR)
library(aod)
library(ggfortify)
library(factoextra)
library(tree)
library(randomForest)
library(ridge)
library(ggplot2)
library(RColorBrewer)
library(corrplot)
library(reshape2)
library(smotefamily)
library(DMwR)
library(pls)
library(bestNormalize)
library(ResourceSelection)
library(ROCR)
library(gplots)
library(ggpubr)
```
## Load in dataset and look at data
```{r}
# import data
#datain <- read.csv("https://raw.githubusercontent.com/lijjumathew/MSDS-Applied-Statistics-Project2/master/Bank_Additional%20_Full.csv", sep=",", header=TRUE)
datain <- read.csv("D:/MS Data Science/SMU/6372 - Applied Stats/Project 2/bank-additional-full.csv", header = TRUE)

# View data
str(datain)
summary(datain)

# convert to dataframe
datain <- as.data.frame(datain)

# Convert all "unknown" to NA
datain[datain == "unknown"] <- NA

# Count NA values in each column
na_count <-sapply(datain, function(y) sum(length(which(is.na(y)))))
na_count <- data.frame(na_count)
na_count

# Count number of yes and no
y_count <-sapply(datain, function(y) sum(length(which(y=="yes"))))
y_count <- data.frame(y_count)
y_count

n_count <-sapply(datain, function(y) sum(length(which(y=="no"))))
n_count <- data.frame(n_count)
n_count

# there are 36,548 No's and 4,640 Yes's.
```
## Impute missing data & convert to numeric
```{r}
# convert to factor for imputation
datain$job <- as.factor(datain$job)
datain$marital <- as.factor(datain$marital)
datain$education <- as.factor(datain$education)
datain$default <- as.factor(datain$default)
datain$housing <- as.factor(datain$housing)
datain$loan <- as.factor(datain$loan)
datain$contact <- as.factor(datain$contact)
datain$month <- dplyr::recode(datain$month, 
                                    "jan"="1",
                                    "feb"="2",
                                    "mar"="3",
                                    "apr"="4",
                                    "may"="5",
                                    "jun"="6",
                                    "jul"="7",
                                    "aug"="8",
                                    "sep"="9",
                                    "oct"="10",
                                    "nov"="11",
                                    "dec"="12")
datain$day_of_week <- dplyr::recode(datain$day_of_week, 
                                    "mon"="1",
                                    "tue"="2",
                                    "wed"="3",
                                    "thu"="4",
                                    "fri"="5",
                                    "sat"="6",
                                    "sun"="7")
datain$day_of_week <- as.factor(datain$day_of_week)
datain$poutcome <- as.factor(datain$poutcome)

# setting up parms
init = mice(datain, maxit=0) 
meth = init$method
predM = init$predictorMatrix

# impute the following columns
meth[c("job", "marital","education","default","housing", "loan")]="polyreg"

# imputate & new dataset
set.seed(123)
imputed = mice(datain, method=meth, predictorMatrix=predM, m=1)
imputed <- complete(imputed)
sapply(imputed, function(x) sum(is.na(x)))

interpretation_data <- imputed

# convert to numeric
imputed$job <- as.numeric(imputed$job)
imputed$marital <- as.numeric(imputed$marital)
imputed$education <- as.numeric(imputed$education)
imputed$default <- as.numeric(imputed$default)
imputed$housing <- as.numeric(imputed$housing)
imputed$loan <- as.numeric(imputed$loan)
imputed$month <- as.numeric(imputed$month)
imputed$contact <- as.numeric(imputed$contact)
imputed$poutcome <- as.numeric(imputed$poutcome)
imputed$day_of_week <- as.numeric(imputed$day_of_week)

EDAD <- imputed

EDAD_conti <- datain[, !sapply(datain, is.factor)]
EDAD_categ <- datain[, sapply(datain, is.factor)]
```
## EDA Histograms
```{r}
EDAD$y <- as.factor(EDAD$y)
EDAD$y <- as.numeric(EDAD$y)

# Histogram of each variable
par(mfrow = c(5, 4))
MASS::truehist(EDAD$age)
MASS::truehist(EDAD$job)
MASS::truehist(EDAD$marital)
MASS::truehist(EDAD$education)
MASS::truehist(EDAD$default)
MASS::truehist(EDAD$housing)
MASS::truehist(EDAD$loan)
MASS::truehist(EDAD$contact)
MASS::truehist(EDAD$month)
MASS::truehist(EDAD$day_of_week)
MASS::truehist(EDAD$duration)
MASS::truehist(EDAD$campaign)
MASS::truehist(EDAD$pdays)
MASS::truehist(EDAD$previous)
MASS::truehist(EDAD$poutcome)
MASS::truehist(EDAD$emp.var.rate)
MASS::truehist(EDAD$cons.price.idx)
MASS::truehist(EDAD$cons.conf.idx)
MASS::truehist(EDAD$euribor3m)
MASS::truehist(EDAD$nr.employed)

```
## EDA Box plots, Scatter plots, Correlation plots
```{r}

# Box plots to find outliers
EDAD_conti <- dplyr::select(EDAD_conti, -11)
boxplot(EDAD_conti, main ='Box plot - all variables')
ggplot(stack(EDAD_conti),aes(x = ind, y = values) ) + geom_boxplot()
boxplot(EDAD_conti$pdays, main ='Box plot for pdays')

# ScatterPlot
pairs(EDAD_conti, pch=19)

#Computing the p value of correlations
cor.mtest <- function(mat, ...) {
  mat <- as.matrix(mat)
  n <- ncol(mat)
  p.mat<- matrix(NA, n, n)
  diag(p.mat) <- 0
  for (i in 1:(n - 1)) {
    for (j in (i + 1):n) {
      tmp <- cor.test(mat[, i], mat[, j], ...)
      p.mat[i, j] <- p.mat[j, i] <- tmp$p.value
    }
  }
  colnames(p.mat) <- rownames(p.mat) <- colnames(mat)
  p.mat
}
p.mat <- cor.mtest(EDAD)
correlation <- cor(EDAD)

# correlation
cor.data <- EDAD
cor.data$y <- as.numeric(as.factor(cor.data$y))
cor(cor.data[-21], cor.data$y) 

# Heat map to find correlation
heatmap.2(correlation,col=redgreen(75), 
          density.info="none", trace="none", dendrogram=c("row"), 
          symm=F,symkey=T,symbreaks=T, scale="none")


ggscatter(EDAD_conti, x = "pdays", y = "previous", 
          title = "Correlation Matrix",
          add = "reg.line", conf.int = TRUE, 
          cor.coef = TRUE, cor.method = "pearson",
          xlab = "Pdays", ylab = "Previous Contacts")
```
## EDA PCA
```{r}
pc.result<-prcomp(EDAD_conti,scale.=TRUE)
pc.result
pc.scores<-pc.result$x
pc.scores<-data.frame(pc.scores)
pc.scores$y<-EDAD$y

# variance
pr_var <- (pc.result$sdev)^2
# % of variance explained
prop_varex <- pr_var/sum(pr_var)
# show percentage of variance of each component
plot(prop_varex, xlab = "Principal Component", ylab = "Proportion of Variance Explained", type = "b" )
# Scree Plot
plot(cumsum(prop_varex), xlab = "Principal Component", ylab = "Cumulative Proportion of Variance Explained", type = "b" )
# first 6 components are responsible for around 90% of variance
cumsum(prop_varex)
# This result means that the majority of information contained in 11 numeric variables can be reduced to 6 principal components.

ggplot(data = pc.scores, aes(x = PC4, y = PC5)) +
  geom_point(aes(col=y), size=1)+
  ggtitle("PCA of Auto")

```
## BestNormalize on Continious variables
```{r}
# The bestnormalize function provides a detailed output describing if/what transformation cna be done to improve the models
bn.age <- bestNormalize(imputed$age)
bn.campaign <- bestNormalize(imputed$campaign)
bn.pdays <- bestNormalize(imputed$pdays)
bn.previous <- bestNormalize(imputed$previous)
bn.emp.var.rate <- bestNormalize(imputed$emp.var.rate)
bn.cons.price.idx <- bestNormalize(imputed$cons.price.idx)   
bn.cons.conf.idx <- bestNormalize(imputed$cons.conf.idx)    
bn.euribor3m <- bestNormalize(imputed$euribor3m)
bn.nr.employed <- bestNormalize(imputed$nr.employed)

# See results
bn.age # transform necessary
bn.campaign # transform necessary
bn.pdays # transform necessary
bn.previous # transform necessary
bn.emp.var.rate # could possibly take sqrt for minor improvement
bn.cons.price.idx # transform necessary
bn.cons.conf.idx # transform necessary
bn.euribor3m # could possibly take box-cox for minor improvement
bn.nr.employed # transform necessary
```
## Summary Table for Continious Varaibles
```{r}
library(qwraps2)
#Summary stats for continuous variables
options(qwraps2_markup = "markdown")
imputed <- as.data.frame(imputed)
continuous_vars <- select(imputed, c(age,duration,campaign,pdays,previous,emp.var.rate,cons.price.idx,cons.conf.idx,euribor3m,nr.employed,y)) #includes y variable (response var.)

continuous_vars <- as.data.frame(continuous_vars)

summary_statistics <-
  list(
    "Age" =
      list(
        "mean (sd)" = ~ qwraps2::mean_sd(age,na_rm = FALSE),
        "median (Q1, Q3)" = ~  qwraps2::median_iqr(age,na_rm = FALSE),
        "min" = ~min(age, na.rm = FALSE),
        "max" = ~max(age, na.rm = FALSE)),
    "Duration" =
      list(
        "mean (sd)" = ~ qwraps2::mean_sd(duration,na_rm = FALSE),
        "median (Q1, Q3)" = ~  qwraps2::median_iqr(duration,na_rm = FALSE),
        "min" = ~min(duration, na.rm = FALSE),
        "max" = ~max(duration, na.rm = FALSE)),
    "Campaign" =
      list(
        "mean (sd)" = ~ qwraps2::mean_sd(campaign,na_rm = FALSE),
        "median (Q1, Q3)" = ~  qwraps2::median_iqr(campaign,na_rm = FALSE),
        "min" = ~min(campaign, na.rm = FALSE),
        "max" = ~max(campaign, na.rm = FALSE)),                
    "Previous Days Since Contacted" =
      list(
        "mean (sd)" = ~ qwraps2::mean_sd(pdays,na_rm = FALSE),
        "median (Q1, Q3)" = ~  qwraps2::median_iqr(pdays,na_rm = FALSE),
        "min" = ~min(pdays, na.rm = FALSE),
        "max" = ~max(pdays, na.rm = FALSE)),                
    "Previous # of contacts performed campaign" =
      list(
        "mean (sd)" = ~ qwraps2::mean_sd(previous,na_rm = FALSE),
        "median (Q1, Q3)" = ~  qwraps2::median_iqr(previous,na_rm = FALSE),
        "min" = ~min(previous, na.rm = FALSE),
        "max" = ~max(previous, na.rm = FALSE)),
    "Employment Variation Rate" =
      list(
        "mean (sd)" = ~ qwraps2::mean_sd(emp.var.rate,na_rm = FALSE),
        "median (Q1, Q3)" = ~  qwraps2::median_iqr(emp.var.rate,na_rm = FALSE),
        "min" = ~min(emp.var.rate, na.rm = FALSE),
        "max" = ~max(emp.var.rate, na.rm = FALSE)),
    "Consumer Price Index" =
      list(
        "mean (sd)" = ~ qwraps2::mean_sd(cons.price.idx,na_rm = FALSE),
        "median (Q1, Q3)" = ~  qwraps2::median_iqr(cons.price.idx,na_rm = FALSE),
        "min" = ~min(cons.price.idx, na.rm = FALSE),
        "max" = ~max(cons.price.idx, na.rm = FALSE)),
    "Consumer Confidence Index" =
      list(
        "mean (sd)" = ~ qwraps2::mean_sd(cons.conf.idx,na_rm = FALSE),
        "median (Q1, Q3)" = ~  qwraps2::median_iqr(cons.conf.idx,na_rm = FALSE),
        "min" = ~min(cons.conf.idx, na.rm = FALSE),
        "max" = ~max(cons.conf.idx, na.rm = FALSE)),
    "Euribor 3-Month Rate" =
      list(
        "mean (sd)" = ~ qwraps2::mean_sd(euribor3m,na_rm = FALSE),
        "median (Q1, Q3)" = ~  qwraps2::median_iqr(euribor3m,na_rm = FALSE),
        "min" = ~min(euribor3m, na.rm = FALSE),
        "max" = ~max(euribor3m, na.rm = FALSE)),
    "Number of Employees" =
      list(
        "mean (sd)" = ~ qwraps2::mean_sd(nr.employed,na_rm = FALSE),
        "median (Q1, Q3)" = ~  qwraps2::median_iqr(nr.employed,na_rm = FALSE),
        "min" = ~min(nr.employed, na.rm = FALSE),
        "max" = ~max(nr.employed, na.rm = FALSE))
  )

#View summary stats grouped by Yes/No
print(qwraps2::summary_table(
  dplyr::group_by(continuous_vars,y),
  summary_statistics),
  rtitle = "Summary Statistics Table for Continuious Variables Grouped by Yes/No Responses")

##pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)
##emp.var.rate: employment variation rate - quarterly indicator (numeric)
##cons.price.idx: consumer price index - monthly indicator (numeric)
##cons.conf.idx: consumer confidence index - monthly indicator (numeric)
##euribor3m: euribor 3 month rate - daily indicator (numeric)
##nr.employed: number of employees - quarterly indicator (numeric)
```

# -----------------------------------------------------------------------------
# -----------------------------------------------------------------------------
# -----------------------------------------------------------------------------

# Objective 1 - Simple Logistic Regression
## Logit - Data splitting
```{r}
set.seed(1234)
## Splitting and Down Sampling
attach(imputed)
dim(imputed)
prop.table(table(imputed$y))
imputed$y <- as.factor(imputed$y)
training.samples <- imputed$y %>%
  createDataPartition(p = 0.7, list = FALSE)
train <- imputed[training.samples, ]
test <- imputed[-training.samples, ]
table(train$y)
prop.table(table(train$y))
table(test$y)
prop.table(table(test$y))

# 60-40 down sampling
library(DMwR)
train_bal_s_60 <- DMwR::SMOTE(form = train$y ~ .,data = train, k = 5, perc.over = 390)
table(train_bal_s_60$y)
prop.table(table(train_bal_s_60$y))
summary(train_bal_s_60)
```
## Logit - Full Model - SMOTE 60 - 40 Sampled
```{r}
set.seed(123)
logit.os.Train <- train_bal_s_60
summary(logit.os.Train)
str(logit.os.Train)
logit.os.Test <- test
logit.os.Train$y <- as.factor(logit.os.Train$y)
attach(logit.os.Train)
# Build Full Model
full.os.log<-glm(y~.,family="binomial",data=logit.os.Train)
# Step Model
step.os.log<-full.os.log %>% stepAIC(trace=FALSE)
# Model Summary
summary(step.os.log)
# Error Metrics
step.os.aic <- step.os.log$aic
step.os.aic
# Confidence Intervals
exp(cbind("Odds ratio" = coef(step.os.log), confint.default(step.os.log, level = 0.95)))
# VIF Scores
vif(step.os.log)
# Predictions & Accuracy
fit.pred.step<-predict(step.os.log,newdata=logit.os.Test,type="response")
cutoff<-0.5
class.step<-factor(ifelse(fit.pred.step>cutoff,"no","yes"),levels=c("yes","no"))
conf.step<-table(class.step,logit.os.Test$y)
print("Confusion matrix for Stepwise")
conf.step
step.os.logit.acc <- sum(diag(conf.step))/sum(conf.step)
step.os.logit.acc
```

1. Using SMOTE 60-40 sampled data as the train data
2. From the full model and the significant predictors from step wise model and taking into account the correlated predictors
below are the some of the models.

## Logit - Reduced Model 6(job,education,campaign,poutcome,nr.employed)
## Accuracy - 80.7%, Sensitivity - 83.7, Specificity - 57.0, MC - 0.1927, AUC - 0.7387
```{r reduced model 6}
set.seed(123)
logit.os.Train <- train_bal_s_60
summary(logit.os.Train)
str(logit.os.Train)
logit.os.Test <- test
logit.os.Train$y <- as.factor(logit.os.Train$y)

# Build Full Model
red.mod6.log <- glm(y ~ job + education  +  campaign  + poutcome + nr.employed , family="binomial",data=logit.os.Train)

# Step Model
step.os6.log <- red.mod6.log %>% stepAIC(trace=FALSE)

# Model Summary
summary(step.os6.log)

# Error Metrics
step.os6.aic <- step.os.log$aic
step.os6.aic

# Confidence Intervals
exp(cbind("Odds ratio" = coef(step.os6.log), confint.default(step.os6.log, level = 0.95)))

# VIF Scores
vif.6 <- vif(step.os6.log)
vif.6.max <- max(vif.6)
vif.6
vif.6.max

# Predictions & Accuracy
fit.pred6.step<-predict(step.os6.log,newdata=logit.os.Test,type="response")
cutoff<-0.5
class.step6<-factor(ifelse(fit.pred6.step>cutoff,"yes","no"))
print("Confusion matrix for Stepwise")
obj1.mod6.cm <- confusionMatrix(as.factor(class.step6), logit.os.Test$y)

obj1.mod6.acc <- obj1.mod6.cm$overall["Accuracy"]
obj1.mod6.acc
obj1.mod6.sens <- obj1.mod6.cm$byClass["Sensitivity"]
obj1.mod6.sens
obj1.mod6.spec <- obj1.mod6.cm$byClass["Specificity"]
obj1.mod6.spec

#MisCalc Rates
obj1.mod6.msclc <- mean(class.step6 != logit.os.Test$y)
print("Miscalculation Rate")
obj1.mod6.msclc

# Roc Curves & value
p.6 <- predict(step.os6.log, newdata=logit.os.Test, type="response")
pr.6 <- prediction(p.6, logit.os.Test$y)
# TPR = sensitivity, FPR=specificity
prf.6 <- performance(pr.6, measure = "tpr", x.measure = "fpr")
auc.6 <- performance(pr.6, measure = "auc")
auc.6 <- auc.6@y.values[[1]]
auc.6
plot(prf.6, main = paste("ROC Curve  \n AUC = ", auc), col = "blue")
```
## Logit - Reduced Model 1(job,month,campaign,pdays,poutcome,emp.var.rate,cons.price.idx) 
## Accuracy - 81.1%, Sensitivity - 84.9, Specificity - 50.9, MC - 0.1886, AUC - 0.7206
```{r reduced model 1}
set.seed(123)
logit.os.Train <- train_bal_s_60
summary(logit.os.Train)
str(logit.os.Train)
logit.os.Test <- test
logit.os.Train$y <- as.factor(logit.os.Train$y)
attach(logit.os.Train)

# Build Full Model
red.mod1.log <- glm(y~ job + month + campaign + pdays + poutcome + emp.var.rate + cons.price.idx ,family="binomial",data=logit.os.Train)

# Step Model
step.os1.log <- red.mod1.log %>% stepAIC(trace=FALSE)

# Model Summary
summary(step.os1.log)

# Error Metrics
step.os1.aic <- step.os1.log$aic
step.os1.aic

# Confidence Intervals
exp(cbind("Odds ratio" = coef(step.os1.log), confint.default(step.os1.log, level = 0.95)))

# VIF Scores
vif.1 <- vif(step.os1.log)
vif.1.max <- max(vif.1)
vif.1
vif.1.max

# Predictions & Accuracy
fit.pred1.step<-predict(step.os1.log,newdata=logit.os.Test, type="response")
cutoff<-0.5
class.1step<-factor(ifelse(fit.pred1.step>cutoff,"yes","no"))
obj1.mod1.cm <- confusionMatrix(as.factor(class.1step), logit.os.Test$y)

obj1.mod1.acc <- obj1.mod1.cm$overall["Accuracy"]
obj1.mod1.acc
obj1.mod1.sens <- obj1.mod1.cm$byClass["Sensitivity"]
obj1.mod1.sens
obj1.mod1.spec <- obj1.mod1.cm$byClass["Specificity"]
obj1.mod1.spec

#MisCalc Rates
obj1.mod1.msclc <- mean(class.1step != logit.os.Test$y)
print("Miscalculation Rate")
obj1.mod1.msclc

# Roc Curves & value
p1 <- predict(step.os1.log, newdata=logit.os.Test, type="response")
pr1 <- prediction(p1, logit.os.Test$y)
# TPR = sensitivity, FPR=specificity
prf1 <- performance(pr1, measure = "tpr", x.measure = "fpr")
auc1 <- performance(pr1, measure = "auc")
auc1 <- auc1@y.values[[1]]
auc1
plot(prf1, main = paste("ROC Curve  \n AUC = ", auc), col = "blue")
```
## Logit - Reduced Model 2(age,job,contact,day_of_week,campaign,pdays,poutcome,emp.var.rate,nr.employed)
## Accuracy - 79.3%, Sensitivity - 85.3, Specificity - 43.5, MC - 0.2068, AUC - 0.7012
```{r reduced model 2}
set.seed(123)
logit.os.Train <- train_bal_s_60
summary(logit.os.Train)
str(logit.os.Train)
logit.os.Test <- test
logit.os.Train$y <- as.factor(logit.os.Train$y)

# Build Full Model
red.mod2.log <- glm(y ~ age + job + contact + day_of_week + campaign + pdays + poutcome + emp.var.rate + nr.employed , family="binomial",data=logit.os.Train)

# Step Model
step.os2.log <- red.mod2.log %>% stepAIC(trace=FALSE)

# Model Summary
summary(step.os2.log)

# Error Metrics
step.os2.aic <- step.os2.log$aic
step.os2.aic

# Confidence Intervals
exp(cbind("Odds ratio" = coef(step.os2.log), confint.default(step.os2.log, level = 0.95)))

# VIF Scores
vif.2 <- vif(step.os2.log)
vif.2.max <- max(vif.2)
vif.2
vif.2.max

# Predictions & Accuracy
fit.pred.step2<-predict(step.os2.log,newdata=logit.os.Test,type="response")
cutoff<-0.5
class.step2<-factor(ifelse(fit.pred.step2>cutoff,"yes","no"))
obj1.mod2.cm <- confusionMatrix(as.factor(class.step2), logit.os.Test$y)

obj1.mod2.acc <- obj1.mod2.cm$overall["Accuracy"]
obj1.mod2.acc
obj1.mod2.sens <- obj1.mod2.cm$byClass["Sensitivity"]
obj1.mod2.sens
obj1.mod2.spec <- obj1.mod2.cm$byClass["Specificity"]
obj1.mod2.spec

#MisCalc Rates
obj1.mod2.msclc <- mean(class.step2 != logit.os.Test$y)
print("Miscalculation Rate")
obj1.mod2.msclc

# Roc Curves & value
p2 <- predict(step.os2.log, newdata=logit.os.Test, type="response")
pr2 <- prediction(p2, logit.os.Test$y)
# TPR = sensitivity, FPR=specificity
prf2 <- performance(pr2, measure = "tpr", x.measure = "fpr")
auc2 <- performance(pr2, measure = "auc")
auc2 <- auc2@y.values[[1]]
auc2
plot(prf2, main = paste("ROC Curve  \n AUC = ", auc), col = "blue")
```
## Logit - Reduced Model 4(job,contact,day_of_week,campaign,pdays,poutcome,euribor3m)
## Accuracy - 80.5%, Sensitivity - 85.4, Specificity - 42.0, MC - 0.1947, AUC - 0.6911
```{r reduced model 4}
set.seed(123)
logit.os.Train <- train_bal_s_60
summary(logit.os.Train)
str(logit.os.Train)
logit.os.Test <- test
logit.os.Train$y <- as.factor(logit.os.Train$y)

# Build Full Model
red.mod4.log <- glm(y ~ job + contact + day_of_week + campaign + pdays + poutcome + euribor3m , family="binomial",data=logit.os.Train)

# Step Model
step.os4.log <- red.mod4.log %>% stepAIC(trace=FALSE)

# Model Summary
summary(step.os4.log)

# Error Metrics
step.os4.aic <- step.os4.log$aic
step.os4.aic

# Confidence Intervals
exp(cbind("Odds ratio" = coef(step.os4.log), confint.default(step.os4.log, level = 0.95)))

# VIF Scores
vif.4 <- vif(step.os4.log)
vif.4.max <- max(vif.4)
vif.4
vif.4.max

# Predictions & Accuracy
fit.pred.step4<-predict(step.os4.log,newdata=logit.os.Test,type="response")
cutoff<-0.5
class.step4<-factor(ifelse(fit.pred.step4>cutoff,"yes","no"))
obj1.mod4.cm <- confusionMatrix(as.factor(class.step2), logit.os.Test$y)

obj1.mod4.acc <- obj1.mod4.cm$overall["Accuracy"]
obj1.mod4.acc
obj1.mod4.sens <- obj1.mod4.cm$byClass["Sensitivity"]
obj1.mod4.sens
obj1.mod4.spec <- obj1.mod4.cm$byClass["Specificity"]
obj1.mod4.spec

#MisCalc Rates
obj1.mod4.msclc <- mean(class.step4 != logit.os.Test$y)
print("Miscalculation Rate")
obj1.mod4.msclc

# Roc Curves & value
p4 <- predict(step.os4.log, newdata=logit.os.Test, type="response")
pr4 <- prediction(p4, logit.os.Test$y)
# TPR = sensitivity, FPR=specificity
prf4 <- performance(pr4, measure = "tpr", x.measure = "fpr")
auc4 <- performance(pr4, measure = "auc")
auc4 <- auc4@y.values[[1]]
auc4
plot(prf4, main = paste("ROC Curve  \n AUC = ", auc), col = "blue")
```
## Logit - Reduced Model 7(job,education,campaign,pdays,poutcome,nr.employed, emp.var.rate)
## Accuracy - 84.2%, Sensitivity - 89.4, Specificity - 44.1, MC - 0.1573, AUC - 0.7119
```{r reduced model 7}
set.seed(123)
logit.os.Train <- train_bal_s_60
summary(logit.os.Train)
str(logit.os.Train)
logit.os.Test <- test
logit.os.Train$y <- as.factor(logit.os.Train$y)

# Build Full Model
red.mod7.log <- glm(y ~ job + education + campaign + pdays + poutcome + nr.employed + emp.var.rate, family="binomial",data=logit.os.Train)

# Step Model
step.os7.log <- red.mod7.log %>% stepAIC(trace=FALSE)

# Model Summary
summary(step.os7.log)

# Error Metrics
step.os7.aic <- step.os7.log$aic
step.os7.aic

# Confidence Intervals
exp(cbind("Odds ratio" = coef(step.os7.log), confint.default(step.os7.log, level = 0.95)))

# VIF Scores
vif.7 <- vif(step.os7.log)
vif.7.max <- max(vif.7)
vif.7
vif.7.max

# Predictions & Accuracy
fit.pred.step7<-predict(step.os7.log,newdata=logit.os.Test,type="response")
cutoff <- 0.5
class.step7 <- factor(ifelse(fit.pred.step7 > cutoff,"yes","no"))
print("Confusion matrix for Stepwise")
obj1.mod7.cm <- confusionMatrix(as.factor(class.step2), logit.os.Test$y)

obj1.mod7.acc <- obj1.mod7.cm$overall["Accuracy"]
obj1.mod7.acc
obj1.mod7.sens <- obj1.mod7.cm$byClass["Sensitivity"]
obj1.mod7.sens
obj1.mod7.spec <- obj1.mod7.cm$byClass["Specificity"]
obj1.mod7.spec

#MisCalc Rates
obj1.mod7.msclc <- mean(class.step7 != logit.os.Test$y)
print("Miscalculation Rate")
obj1.mod7.mscl

# Roc Curves & value
p7 <- predict(step.os7.log, newdata=logit.os.Test, type="response")
pr7 <- prediction(p7, logit.os.Test$y)
# TPR = sensitivity, FPR=specificity
prf7 <- performance(pr7, measure = "tpr", x.measure = "fpr")
auc7 <- performance(pr7, measure = "auc")
auc7 <- auc7@y.values[[1]]
auc7
plot(prf7, main = paste("ROC Curve  \n AUC = ", auc), col = "blue")
```
## Logit - Reduced Model 3(job,contact,day_of_week,campaign,pdays,poutcome,euribor3m,nr.employed)
## Accuracy - 79.99%, Sensitivity - 84.3, Specificity - 45.7, VIF ISSUES!
```{r reduced model 3}
set.seed(1234)
logit.os.Train <- train_bal_s_60
summary(logit.os.Train)
str(logit.os.Train)
logit.os.Test <- test
logit.os.Train$y <- as.factor(logit.os.Train$y)

# Build Full Model
red.mod3.log <- glm(y ~ job + contact + day_of_week + campaign + pdays + poutcome + euribor3m + nr.employed , family="binomial",data=logit.os.Train)

# Step Model
step.os3.log <- red.mod3.log %>% stepAIC(trace=FALSE)

# Model Summary
summary(step.os3.log)

# Error Metrics
step.os3.aic <- step.os3.log$aic
step.os3.aic

# Confidence Intervals
exp(cbind("Odds ratio" = coef(step.os3.log), confint.default(step.os3.log, level = 0.95)))

# VIF Scores
vif.3 <- vif(step.os3.log)
vif.3.max <- max(vif.3)
vif.3
vif.3.max

# Predictions & Accuracy
fit.pred.step3<-predict(step.os3.log,newdata=logit.os.Test,type="response")
cutoff<-0.5
class.step3<-factor(ifelse(fit.pred.step3>cutoff,"yes","no"))
obj1.mod3.cm <- confusionMatrix(as.factor(class.step3), logit.os.Test$y)

obj1.mod3.acc <- obj1.mod3.cm$overall["Accuracy"]
obj1.mod3.acc
obj1.mod3.sens <- obj1.mod3.cm$byClass["Sensitivity"]
obj1.mod3.sens
obj1.mod3.spec <- obj1.mod3.cm$byClass["Specificity"]
obj1.mod3.spec

#MisCalc Rates
obj1.mod3.msclc <- mean(class.step3 != logit.os.Test$y)
print("Miscalculation Rate")
obj1.mod3.mscl

# Roc Curves & value
p3 <- predict(step.os3.log, newdata=logit.os.Test, type="response")
pr3 <- prediction(p3, logit.os.Test$y)
# TPR = sensitivity, FPR=specificity
prf3 <- performance(pr3, measure = "tpr", x.measure = "fpr")
auc3 <- performance(pr3, measure = "auc")
auc3 <- auc3@y.values[[1]]
auc3
plot(prf3, main = paste("ROC Curve  \n AUC = ", auc), col = "blue")
```
## Logit - Reduced Model 5(job,contact,education,day_of_week,campaign,pdays,poutcome,euribor3m,nr.employed)
## Accuracy - 80.1%, Sensitivity - 84.5, Specificity - 45.1, VIF ISSUES!
```{r reduced model 5}
set.seed(1234)
logit.os.Train <- train_bal_s_60
summary(logit.os.Train)
str(logit.os.Train)
logit.os.Test <- test
logit.os.Train$y <- as.factor(logit.os.Train$y)

# Build Full Model
red.mod5.log <- glm(y ~ job + education + contact + day_of_week + campaign + pdays + poutcome + euribor3m + nr.employed , family="binomial",data=logit.os.Train)

# Step Model
step.os5.log <- red.mod5.log %>% stepAIC(trace=FALSE)

# Model Summary
summary(step.os5.log)

# Error Metrics
step.os5.aic <- step.os5.log$aic
step.os5.aic

# Confidence Intervals
exp(cbind("Odds ratio" = coef(step.os5.log), confint.default(step.os5.log, level = 0.95)))

# VIF Scores
vif.5 <- vif(step.os5.log)
vif.5.max <- max(vif.5)
vif.5
vif.5.max

# Predictions & Accuracy
fit.pred.step5<-predict(step.os5.log,newdata=logit.os.Test,type="response")
cutoff<-0.5
class.step5<-factor(ifelse(fit.pred.step5>cutoff,"yes","no"))
obj1.mod5.cm <- confusionMatrix(as.factor(class.step5), logit.os.Test$y)

obj1.mod5.acc <- obj1.mod5.cm$overall["Accuracy"]
obj1.mod5.acc
obj1.mod5.sens <- obj1.mod5.cm$byClass["Sensitivity"]
obj1.mod5.sens
obj1.mod5.spec <- obj1.mod5.cm$byClass["Specificity"]
obj1.mod5.spec

#MisCalc Rates
obj1.mod5.msclc <- mean(class.step5 != logit.os.Test$y)
print("Miscalculation Rate")
obj1.mod5.mscl

# Roc Curves & value
p5 <- predict(step.os5.log, newdata=logit.os.Test, type="response")
pr5 <- prediction(p5, logit.os.Test$y)
# TPR = sensitivity, FPR=specificity
prf5 <- performance(pr5, measure = "tpr", x.measure = "fpr")
auc5 <- performance(pr5, measure = "auc")
auc5 <- auc5@y.values[[1]]
auc5
plot(prf5, main = paste("ROC Curve  \n AUC = ", auc), col = "blue")
```

## Logit - Objective 1 Model - Interpretation (job,education,campaign,poutcome,nr.employed)
```{r Objective1 model interpretation}

set.seed(1234)
## Splitting and Down Sampling
interpretation_data$y <- as.factor(interpretation_data$y)
training.samples <- interpretation_data$y %>%
  createDataPartition(p = 0.7, list = FALSE)
train <- interpretation_data[training.samples, ]
test <- interpretation_data[-training.samples, ]

# 60-40 down sampling
library(DMwR)
train_bal_s_60 <- DMwR::SMOTE(form = train$y ~ .,data = train, k = 5, perc.over = 390)
table(train_bal_s_60$y)
prop.table(table(train_bal_s_60$y))
summary(train_bal_s_60)

logit.os.Train <- train_bal_s_60
summary(logit.os.Train)
str(logit.os.Train)
logit.os.Test <- test
logit.os.Train$y <- as.factor(logit.os.Train$y)

# Build Full Model
red.mod6.log <- glm(y ~ job + education  +  campaign  + poutcome + nr.employed , family="binomial",data=logit.os.Train)

# Step Model
step.os6.log <- red.mod6.log %>% stepAIC(trace=FALSE)

# Model Summary
summary(step.os6.log)

# Error Metrics
step.os6.aic <- step.os.log$aic
step.os6.aic

# Confidence Intervals
exp(cbind("Odds ratio" = coef(step.os6.log), confint.default(step.os6.log, level = 0.95)))

# VIF Scores
vif.6 <- vif(step.os6.log)
vif.6.max <- max(vif.6)
vif.6
vif.6.max

# Predictions & Accuracy
fit.pred6.step<-predict(step.os6.log,newdata=logit.os.Test,type="response")
cutoff<-0.5
class.step6<-factor(ifelse(fit.pred6.step>cutoff,"yes","no"))
print("Confusion matrix for Stepwise")
obj1.mod6.cm <- confusionMatrix(as.factor(class.step6), logit.os.Test$y)
obj1.mod6.cm

obj1.mod6.acc <- obj1.mod6.cm$overall["Accuracy"]
obj1.mod6.acc
obj1.mod6.sens <- obj1.mod6.cm$byClass["Sensitivity"]
obj1.mod6.sens
obj1.mod6.spec <- obj1.mod6.cm$byClass["Specificity"]
obj1.mod6.spec

#MisCalc Rates
obj1.mod6.msclc <- mean(class.step6 != logit.os.Test$y)
print("Miscalculation Rate")
obj1.mod6.msclc

# Roc Curves & value
p.6 <- predict(step.os6.log, newdata=logit.os.Test, type="response")
pr.6 <- prediction(p.6, logit.os.Test$y)
# TPR = sensitivity, FPR=specificity
prf.6 <- performance(pr.6, measure = "tpr", x.measure = "fpr")
auc.6 <- performance(pr.6, measure = "auc")
auc.6 <- auc.6@y.values[[1]]
auc.6
plot(prf.6, main = paste("ROC Curve  \n AUC = ", auc), col = "blue")
```
# Model 6 is the best model given the specifity value

# -----------------------------------------------------------------------------
# -----------------------------------------------------------------------------
# -----------------------------------------------------------------------------

# Objective 2 - LDA/QDA & Other Models to build on Objective 1 Model

## ANOVA for obj 1 models
```{r}
# Anova Table
res.aov2 <- aov(as.numeric(y) ~ job + education + campaign + poutcome + nr.employed, data = imputed)
summary(res.aov2)
# There is a significant differnece in means
```

## Logit Round 2 - Interactive Model (job,education,campaign,poutcome,nr.employed)
## Objective 1 model with added complexity
## Accuracy - 79.5%, Sensitivity - 82.2, Specificity - 57.5, MC - 0.20547, AUC - 0.7385
```{r Objective 1 complex model}
set.seed(123)

train_bal_s_60$job <- as.numeric(train_bal_s_60$job)
train_bal_s_60$marital <- as.numeric(train_bal_s_60$marital)
train_bal_s_60$education <- as.numeric(train_bal_s_60$education)
train_bal_s_60$default <- as.numeric(train_bal_s_60$default)
train_bal_s_60$housing <- as.numeric(train_bal_s_60$housing)
train_bal_s_60$loan <- as.numeric(train_bal_s_60$loan)
train_bal_s_60$month <- as.numeric(train_bal_s_60$month)
train_bal_s_60$contact <- as.numeric(train_bal_s_60$contact)
train_bal_s_60$poutcome <- as.numeric(train_bal_s_60$poutcome)
train_bal_s_60$day_of_week <- as.numeric(train_bal_s_60$day_of_week)

logit.os.Train <- train_bal_s_60
summary(logit.os.Train)
str(logit.os.Train)
logit.os.Test <- test
logit.os.Test$job <- as.numeric(logit.os.Test$job)
logit.os.Test$marital <- as.numeric(logit.os.Test$marital)
logit.os.Test$education <- as.numeric(logit.os.Test$education)
logit.os.Test$default <- as.numeric(logit.os.Test$default)
logit.os.Test$housing <- as.numeric(logit.os.Test$housing)
logit.os.Test$loan <- as.numeric(logit.os.Test$loan)
logit.os.Test$month <- as.numeric(logit.os.Test$month)
logit.os.Test$contact <- as.numeric(logit.os.Test$contact)
logit.os.Test$poutcome <- as.numeric(logit.os.Test$poutcome)
logit.os.Test$day_of_week <- as.numeric(logit.os.Test$day_of_week)

logit.os.Train$y <- as.factor(logit.os.Train$y)

# Analyzing interaction terms to determine ideal ones to put in the model
# The ^2 runs the two way anova on all the variables
lm.fit2=lm(as.numeric(y) ~ (job + education + campaign + poutcome + nr.employed)^2,data=logit.os.Train)
anova(lm.fit2)

# Build Full Model
red.mod7.log <- glm(y ~ job + education + campaign + poutcome +
                      job*education + job*campaign + education*campaign  + campaign*nr.employed + 
                      education*nr.employed + education*poutcome + poutcome*nr.employed, family="binomial", data=logit.os.Train)

# Step Model
step.osj2.log <- red.mod7.log %>% stepAIC(trace=FALSE)

# Model Summary
summary(step.osj2.log)

# Error Metrics
step.osj2.aic <- step.osj2.log$aic
step.osj2.aic

# Confidence Intervals
exp(cbind("Odds ratio" = coef(step.osj2.log), confint.default(step.osj2.log, level = 0.95)))

# VIF Scores
vif.osj2.log <- vif(step.osj2.log)

# Predictions & Accuracy
fit.pred.step.oj2 <-predict(step.osj2.log,newdata=logit.os.Test,type="response")
cutoff <- 0.5
class.step.oj2 <- factor(ifelse(fit.pred.step.oj2 > cutoff,"yes","no"))
print("Confusion matrix for Stepwise")
obj1.modoj2.cm <- confusionMatrix(as.factor(class.step.oj2), logit.os.Test$y)
obj1.modoj2.cm

obj1.modoj2.acc <- obj1.modoj2.cm$overall["Accuracy"]
obj1.modoj2.acc
obj1.modoj2.sens <- obj1.modoj2.cm$byClass["Sensitivity"]
obj1.modoj2.sens
obj1.modoj2.spec <- obj1.modoj2.cm$byClass["Specificity"]
obj1.modoj2.spec

#MisCalc Rates
misClasificError <- mean(class.step.oj2 != logit.os.Test$y)
print("Miscalculation Rate")
misClasificError

# Roc Curves & value
p <- predict(step.os.log, newdata=logit.os.Test, type="response")
pr <- prediction(p, logit.os.Test$y)
# TPR = sensitivity, FPR=specificity
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
auc
plot(prf, main = paste("ROC Curve  \n AUC = ", auc), col = "blue")

# Leverage plots
par(mfrow=c(2,2))
plot(step.osj2.log)
```
## This model gains some specificity over the original model due to the interaction terms added

# --------------------------------------------------------

## Final Datasets to be used in the rest of the models
## Variables used determined by signficant variables defined by stepwise
## LDA/QDA Data
```{r}
set.seed(123)

# Select transformations
EDAD2 <- EDAD
EDAD2$pdays <- ifelse(EDAD2$pdays == 999, -1, EDAD2$pdays)

# EDA dataset - narrowed down to columns to be used in regression
edadata <- EDAD2
edadata <- edadata %>%
  select(age, job, marital, education, contact, month, day_of_week, campaign, pdays, previous,
         poutcome, cons.price.idx, cons.conf.idx, y)

obj2.data <- edadata

# LDA & QDA Data
EDAD3 <- EDAD
EDAD3$pdays <- ifelse(EDAD3$pdays == 999, -1, EDAD3$pdays)

edadata2 <- EDAD3
edadata2 <- edadata2 %>%
  select(age, campaign, pdays, previous, emp.var.rate, cons.price.idx, cons.conf.idx, euribor3m, nr.employed, y)
edadata2 <- japply(edadata2, which(sapply(edadata2, class)=="integer"), as.numeric )

lda.qda.data <- edadata2

# Original Data Split
lqda.orig.index <- sample(1:nrow(lda.qda.data), 0.7*nrow(lda.qda.data))  # row indices for training data
lqda.orig.train <- lda.qda.data[lqda.orig.index, ]  # model training data
lqda.orig.test  <- lda.qda.data[-lqda.orig.index, ]   # test data


# Balanced Data Set
l.BalanceIndex <- createDataPartition(lda.qda.data$y, p = .7,
                                      list = FALSE,
                                      times = 1)
#Seperate all 4640 "yes" in dataset
lY_data <- filter(lda.qda.data,grepl("yes",y))

#Seperate all "no's" in dataset
lN_data <- filter(lda.qda.data,grepl("no", y))

#random sample 4640 no's from no-only dataset
lN_data <- lN_data[sample(nrow(lN_data),4640),]

#Add the 4640 yes dataset with the random sampled 4640 no's dataset  
l.balanced_split_data <- rbind(lY_data,lN_data)
l.balanced_split_data$y <- as.factor(l.balanced_split_data$y)

lq.balanced.train <- l.balanced_split_data
lq.balanced.test <- lda.qda.data[-l.BalanceIndex,]
lq.balanced.test$y <- as.factor(lq.balanced.test$y)



# Rose Data
lRoseindex <- sample(1:nrow(lda.qda.data), 0.7*nrow(lda.qda.data))  # row indices for training data
lq.rose.train <- ROSE(y ~ .,data = lda.qda.data[lRoseindex, ])$data
lq.rose.train$y <- as.factor(lq.rose.train$y)
lq.rose.test  <- lda.qda.data[-lRoseindex, ]   # test data
lq.rose.test$y <- as.factor(lq.rose.test$y)
```

# Data Splitting
```{r}
# Original Data Split
orig.index <- sample(1:nrow(obj2.data), 0.7*nrow(obj2.data))  # row indices for training data
orig.train <- obj2.data[orig.index, ]  # model training data
orig.test  <- obj2.data[-orig.index, ]   # test data



# Balanced Data Set
BalanceIndex <- createDataPartition(obj2.data$y, p = .7,
                                    list = FALSE,
                                    times = 1)
#Seperate all 4640 "yes" in dataset
Y_data <- filter(obj2.data,grepl("yes",y))

#Seperate all "no's" in dataset
N_data <- filter(obj2.data,grepl("no",y))

#random sample 4640 no's from no-only dataset
N_data <- N_data[sample(nrow(N_data),4640),]

#Add the 4640 yes dataset with the random sampled 4640 no's dataset  
balanced_split_data <- rbind(Y_data,N_data)
balanced_split_data$y <- as.factor(balanced_split_data$y)

balanced.train <- balanced_split_data
balanced.train$y <- as.factor(balanced.train$y)
balanced.test <- obj2.data[-BalanceIndex,]
balanced.test$y <- as.factor(balanced.test$y)


# Rose Data
Roseindex <- sample(1:nrow(obj2.data), 0.7*nrow(obj2.data))  # row indices for training data
rose.train <- ROSE(y ~ .,data = obj2.data[Roseindex, ])$data
rose.test  <- obj2.data[-Roseindex, ]   # test data


# Random Forest Data
# Select transformations
RF.DATA <- interpretation_data
RF.DATA$pdays <- ifelse(RF.DATA$pdays == 999, -1, RF.DATA$pdays)

rfdata <- RF.DATA
rfdata  <- rfdata  %>%
  select(age, job, marital, education, contact, month, day_of_week, campaign, pdays, previous,
         poutcome, cons.price.idx, cons.conf.idx, y)

rf.data.final <- rfdata 
rf.data.final$month <- as.factor(rf.data.final$month)
rf.data.final$y <- as.factor(rf.data.final$y)


# Original Data Split
RF.orig.index <- sample(1:nrow(rf.data.final), 0.7*nrow(rf.data.final))  # row indices for training data
RF.orig.train <- rf.data.final[orig.index, ]  # model training data
RF.orig.test  <- rf.data.final[-orig.index, ]   # test data

# Balanced Data Set
rf.BalanceIndex <- createDataPartition(rf.data.final$y, p = .7,
                                    list = FALSE,
                                    times = 1)
#Seperate all 4640 "yes" in dataset
RY_data <- filter(rf.data.final,grepl("yes",y))

#Seperate all "no's" in dataset
RN_data <- filter(rf.data.final,grepl("no",y))

#random sample 4640 no's from no-only dataset
RN_data <- RN_data[sample(nrow(RN_data),4640),]

#Add the 4640 yes dataset with the random sampled 4640 no's dataset  
RF.balanced_split_data <- rbind(RY_data,RN_data)
RF.balanced_split_data$y <- as.factor(RF.balanced_split_data$y)

RF.balanced.train <- RF.balanced_split_data
RF.balanced.train$y <- as.factor(RF.balanced.train$y)
RF.balanced.test <- rf.data.final[-rf.BalanceIndex,]
RF.balanced.test$y <- as.factor(RF.balanced.test$y)

# Rose Data
RF.Roseindex <- sample(1:nrow(rf.data.final), 0.7*nrow(rf.data.final))  # row indices for training data
RF.rose.train <- ROSE(y ~ .,data = rf.data.final[RF.Roseindex, ])$data
RF.rose.test  <- rf.data.final[-RF.Roseindex, ]   # test data


```

# --------------------------------------------------------

## LDA with original... Continious ONLY!
## Accuracy - 89.1%, Sensitivity - 95.9%, Specificity - 32.8%
```{r}
set.seed(123)

o.preproc.train <- lqda.orig.train[,-10]

# Estimate preprocessing parameters
o.preproc.param <- o.preproc.train %>% 
  preProcess(method = c("center", "scale"))

# Transform the data using the estimated parameters
LDA.o.train.transformed <- o.preproc.param %>% predict(lqda.orig.train)
LDA.o.train.transformed$y <- as.factor(LDA.o.train.transformed$y)
LDA.o.test.transformed <- o.preproc.param %>% predict(lqda.orig.test)
LDA.o.test.transformed$y <- as.factor(LDA.o.test.transformed$y)

# Model
lda.o.model <- lda(y ~ ., data = LDA.o.train.transformed)
lda.o.model

# Make predictions
lda.o.predictions <- lda.o.model %>% predict(LDA.o.test.transformed)
lda.o.predictions$class <- as.factor(lda.o.predictions$class)

# Error
lda.o.error <- mean(lqda.orig.test$y != lda.o.predictions$class)
lda.o.error

# Confusion Matrix
lda.o.cm <- caret::confusionMatrix(as.factor(lda.o.predictions$class), as.factor(lqda.orig.test$y))
lda.o.cm

# Accuracy
lda.o.acc <- lda.o.cm$overall["Accuracy"]
lda.o.acc

# Sensitivity and Specificity
lda.o.sens <- lda.o.cm$byClass["Sensitivity"]
lda.o.sens 
lda.o.spec <- lda.o.cm$byClass["Specificity"]
lda.o.spec
```
## LDA with balanced... Continious ONLY!
## Accuracy - 72.7%, Sensitivity - 72.9%, Specificity - 70.7%
```{r}
set.seed(123)

b.preproc.train <- lq.balanced.train[,-10]

# Estimate preprocessing parameters
b.preproc.param <- b.preproc.train %>% 
  preProcess(method = c("center", "scale"))

# Transform the data using the estimated parameters
LDA.b.train.transformed <- b.preproc.param %>% predict(lq.balanced.train)
LDA.b.train.transformed$y <- as.factor(LDA.b.train.transformed$y)
LDA.b.test.transformed <- b.preproc.param %>% predict(lq.balanced.test)
LDA.b.test.transformed$y <- as.factor(LDA.b.test.transformed$y)

# Model
lda.b.model <- lda(y ~ ., data = LDA.b.train.transformed)
lda.b.model

# Make predictions
lda.b.predictions <- lda.b.model %>% predict(LDA.b.test.transformed)
lda.b.predictions$class <- as.factor(lda.b.predictions$class)

# Error
lda.b.error <- mean(lq.balanced.test$y != lda.b.predictions$class)
lda.b.error

# Confusion Matrix
lda.b.cm <- caret::confusionMatrix(as.factor(lda.b.predictions$class), as.factor(lq.balanced.test$y))
lda.b.cm

# Accuracy
lda.b.acc <- lda.b.cm$overall["Accuracy"]
lda.b.acc

# Sensitivity and Specificity
lda.b.sens <- lda.b.cm$byClass["Sensitivity"]
lda.b.sens
lda.b.spec <- lda.b.cm$byClass["Specificity"]
lda.b.spec
```
## LDA with rose... Continious ONLY!
## Accuracy - 71.2%, Sensitivity - 71.1%, Specificity - 71.4%
```{r}
set.seed(123)

r.preproc.train <- lq.rose.train[,-10]

# Estimate preprocessing parameters
r.preproc.param <- r.preproc.train %>% 
  preProcess(method = c("center", "scale"))

# Transform the data using the estimated parameters
LDA.r.train.transformed <- r.preproc.param %>% predict(lq.rose.train)
LDA.r.test.transformed <- r.preproc.param %>% predict(lq.rose.test)

# Model
lda.r.model <- lda(y ~ ., data = LDA.r.train.transformed)
lda.r.model

# Make predictions
lda.r.predictions <- lda.r.model %>% predict(LDA.r.test.transformed)

# Error
lda.r.error <- mean(lq.rose.test$y != lda.r.predictions$class)
lda.r.error

# Confusion Matrix
lda.r.cm <- caret::confusionMatrix(lda.r.predictions$class, as.factor(lq.rose.test$y))
lda.r.cm

# Accuracy
lda.r.acc <- lda.r.cm$overall["Accuracy"]
lda.r.acc

# Sensitivity and Specificity
lda.r.sens <- lda.r.cm$byClass["Sensitivity"]
lda.r.sens
lda.r.spec <- lda.r.cm$byClass["Specificity"]
lda.r.spec
```

# --------------------------------------------------------

# QDA with original... Continious ONLY!
## Accuracy - 87.5%, Sensitivity - 92.2, Specificity - 49.1
```{r}
set.seed(123)

# Fit the model
qda.o.model <- qda(y ~ ., data = lqda.orig.train)
qda.o.model

# Make predictions
qda.o.predictions <- qda.o.model %>% predict(lqda.orig.test)

# Error
qda.o.error <- mean(lqda.orig.test$y != qda.o.predictions$class)
qda.o.error

# Confusion Matrix
qda.o.cm <- caret::confusionMatrix(as.factor(qda.o.predictions$class), as.factor(lqda.orig.test$y))
qda.o.cm

# Model accuracy
qda.o.acc <- qda.o.cm$overall["Accuracy"]
qda.o.acc

# Sensitivity and Specificity
qda.o.sens <- qda.o.cm$byClass["Sensitivity"]
qda.o.sens
qda.o.spec <- qda.o.cm$byClass["Specificity"]
qda.o.spec
```
## QDA with balanced... Continious ONLY!
## Accuracy - 85.7%, Sensitivity - 89.9, Specificity - 52.4
```{r}
set.seed(123)

# Fit the model
qda.b.model <- qda(y ~ ., data = lq.balanced.train)
qda.b.model

# Make predictions
qda.b.predictions <- qda.b.model %>% predict(lq.balanced.test)

# Error
qda.b.error <- mean(lq.balanced.test$y != qda.b.predictions$class)
qda.b.error

# Confusion Matrix
qda.b.cm <- caret::confusionMatrix(as.factor(qda.b.predictions$class), as.factor(lq.balanced.test$y))
qda.b.cm

# Model accuracy
qda.b.acc <- qda.b.cm$overall["Accuracy"]
qda.b.acc

# Sensitivity and Specificity
qda.b.sens <- qda.b.cm$byClass["Sensitivity"]
qda.b.sens
qda.b.spec <- qda.b.cm$byClass["Specificity"]
qda.b.spec
```
## QDA with rose... Continious ONLY!
## Accuracy - 87.3%, Sensitivity - 92.7, Specificity - 44.7
```{r}
set.seed(123)

# Fit the model
qda.r.model <- qda(y ~ ., data = lq.rose.train)
qda.r.model

# Make predictions
qda.r.predictions <- qda.r.model %>% predict(lq.rose.test)

# Error
qda.r.error <- mean(lq.rose.test$y != qda.r.predictions$class)
qda.r.error

# Confusion Matrix
qda.r.cm <- caret::confusionMatrix(as.factor(qda.r.predictions$class), as.factor(lq.rose.test$y))
qda.r.cm

# Model accuracy
qda.r.acc <- qda.r.cm$overall["Accuracy"]
qda.r.acc

# Sensitivity and Specificity
qda.r.sens <- qda.r.cm$byClass["Sensitivity"]
qda.r.sens
qda.r.spec <- qda.r.cm$byClass["Specificity"]
qda.r.spec
```

# --------------------------------------------------------

## MDA with original
## Accuracy - 89.8%, Sensitivity - 98.2%, Specificity - 21.1%
```{r}
set.seed(123)

# Fit the model
mda.o.model <- mda(y ~ ., data = orig.train)
mda.o.model

# Make predictions
mda.o.predictions <- mda.o.model %>% predict(orig.test)

# Error
mda.o.error <- mean(orig.test$y != mda.o.predictions)
mda.o.error

# Confusion Matrix
mda.o.cm <- caret::confusionMatrix(mda.o.predictions, as.factor(orig.test$y))
mda.o.cm

# Model accuracy
mda.o.acc <- mda.o.cm$overall["Accuracy"]
mda.o.acc

# Sensitivity and Specificity
mda.o.sens <- mda.o.cm$byClass["Sensitivity"]
mda.o.sens
mda.o.spec <- mda.o.cm$byClass["Specificity"]
mda.o.spec
```
## MDA with balanced
## Accuracy - 67.8%, Sensitivity - 68.4%, Specificity - 63.2%
```{r}
set.seed(123)

# Fit the model
mda.b.train <- balanced.train
mda.b.model <- mda(y ~ ., data = balanced.train)
mda.b.model

# Make predictions
mda.b.predictions <- mda.b.model %>% predict(balanced.test)

# Error
mda.b.error <- mean(as.numeric(balanced.test$y) != as.numeric(mda.b.predictions))
mda.b.error

# Confusion Matrix
mda.b.cm <- caret::confusionMatrix(as.factor(mda.b.predictions), balanced.test$y)
mda.b.cm

# Model accuracy
mda.b.acc <- mda.b.cm$overall["Accuracy"]
mda.b.acc

# Sensitivity and Specificity
mda.b.sens <- mda.b.cm$byClass["Sensitivity"]
mda.b.sens
mda.b.spec <- mda.b.cm$byClass["Specificity"]
mda.b.spec
```
## MDA with rose
## Accuracy - 76.6%, Sensitivity - 79.9%, Specificity - 52.4%
```{r}
set.seed(123)

# Fit the model
mda.r.model <- mda(y ~ ., data = rose.train)
mda.r.model

# Make predictions
mda.r.predictions <- mda.r.model %>% predict(rose.test)

# Error
mda.r.error <- mean(rose.test$y != mda.r.predictions )
mda.r.error

# Confusion Matrix
mda.r.cm <- caret::confusionMatrix(mda.r.predictions, as.factor(rose.test$y))
mda.r.cm

# Model accuracy
mda.r.acc <- mda.r.cm$overall["Accuracy"]
mda.r.acc

# Sensitivity and Specificity
mda.r.sens <- mda.r.cm$byClass["Sensitivity"]
mda.r.sens
mda.r.spec <- mda.r.cm$byClass["Specificity"]
mda.r.spec
```

# --------------------------------------------------------

## FDA with original
## Accuracy - 90.1%, Sensitivity - 98.4%, Specificity - 20.6%
```{r}
set.seed(123)

# Fit the model
fda.o.model <- fda(y ~ ., data = orig.train)

# Make predictions
fda.o.predictions <- fda.o.model %>% predict(orig.test)

# Error
fda.o.error <- mean(orig.test$y != fda.o.predictions)
fda.o.error

# Confusion Matrix
fda.o.cm <- caret::confusionMatrix(fda.o.predictions, as.factor(orig.test$y))
fda.o.cm

# Model accuracy
fda.o.acc <- fda.o.cm$overall["Accuracy"]
fda.o.acc

# Sensitivity and Specificity
fda.o.sens <- fda.o.cm$byClass["Sensitivity"]
fda.o.sens
fda.o.spec <- fda.o.cm$byClass["Specificity"]
fda.o.spec
```
## FDA with balanced
## Accuracy - 67.5%, Sensitivity - 67.9%, Specificity - 64.1%
```{r}
set.seed(123)

# Fit the model
fda.b.model <- fda(y ~ ., data = balanced.train)

# Make predictions
fda.b.predictions <- fda.b.model %>% predict(balanced.test)

# Error
fdab.error <- mean(balanced.test$y != fda.b.predictions)
fdab.error

# Confusion Matrix
fda.b.cm <- caret::confusionMatrix(fda.b.predictions, as.factor(balanced.test$y))
fda.b.cm

# Model accuracy
fda.b.acc <- fda.b.cm$overall["Accuracy"]
fda.b.acc

# Sensitivity and Specificity
fda.b.sens <- fda.b.cm$byClass["Sensitivity"]
fda.b.sens
fda.b.spec <- fda.b.cm$byClass["Specificity"]
fda.b.spec
```
## FDA with rose
## Accuracy - 68.2%, Sensitivity - 69.1%, Specificity - 61.3%
```{r}
set.seed(123)

# Fit the model
fda.r.model <- fda(y ~ ., data = rose.train)

# Make predictions
fda.r.predictions <- fda.r.model %>% predict(rose.test)

# Error
fda.r.error <- mean(rose.test$y != fda.r.predictions )
fda.r.error

# Confusion Matrix
fda.r.cm <- caret::confusionMatrix(fda.r.predictions, as.factor(rose.test$y))
fda.r.cm

# Model accuracy
fda.r.acc <- fda.r.cm$overall["Accuracy"]
fda.r.acc

# Sensitivity and Specificity
fda.r.sens <- fda.r.cm$byClass["Sensitivity"]
fda.r.sens
fda.r.spec <- fda.r.cm$byClass["Specificity"]
fda.r.spec
```

# --------------------------------------------------------

## RF with original
## Accuracy - 89.6%, Sensitivity - 98.7%, Specificity - 19.3%
```{r}
set.seed(123)

# Converting ‘y’ to a factor
rf.o.train.data <- RF.orig.train
rf.o.train.data$y <- factor(rf.o.train.data$y)
rf.o.test.data <- RF.orig.test
rf.o.test.data$y <- as.factor(rf.o.test.data$y)

# set cross validation parameters
trControl <- trainControl(method = "cv",
                          number = 10,
                          search = "grid")

# Training using ‘random forest’ algorithm
rf.o.model <- train(y~.,data = rf.o.train.data, method = "rf", 
                    metric = "Accuracy", importance = TRUE,
                    trControl = trControl,  ntree = 12)

print(rf.o.model)
plot(rf.o.model)

rf.o.model$bestTune$mtry
rf.o.acc <- max(rf.o.model$results$Accuracy)
rf.o.acc

# Predict
rf.o.predictions <- rf.o.model %>% predict(rf.o.test.data)

# Error
rf.o.error <- mean(rf.o.test.data$y != rf.o.predictions)
rf.o.error

# Confusion Matrix
rf.o.cm <- caret::confusionMatrix(rf.o.predictions, as.factor(rf.o.test.data$y))
rf.o.cm

# Model accuracy
rf.o.acc <- rf.o.cm$overall["Accuracy"]
rf.o.acc

# Sensitivity and Specificity
rf.o.sens <- rf.o.cm$byClass["Sensitivity"]
rf.o.sens
rf.o.spec <- rf.o.cm$byClass["Specificity"]
rf.o.spec
```
## RF with balanced
## Accuracy - 85.4%, Sensitivity - 88.7%, Specificity - 63.1%
```{r}
set.seed(123)

rf.b.train.data <- RF.balanced.train
rf.b.train.data$y <- factor(rf.b.train.data$y)
rf.b.test.data <- RF.balanced.test
rf.b.test.data$y <- as.factor(rf.b.test.data$y)

# set cross validation parameters
trControl <- trainControl(method = "cv",
                          number = 10,
                          search = "grid")

# Training using ‘random forest’ algorithm
rf.b.model <- train(y~.,data = rf.b.train.data, method = "rf", 
                    metric = "Accuracy", importance = TRUE,
                    trControl = trControl, ntree = 24)

print(rf.b.model)
plot(rf.b.model)

rf.b.model$bestTune$mtry

# Predict
rf.b.predictions <- rf.b.model %>% predict(rf.b.test.data)

# Error
rf.b.error <- mean(rf.b.test.data$y != rf.b.predictions)
rf.b.error

# Confusion Matrix
rf.b.cm <- caret::confusionMatrix(rf.b.predictions, as.factor(rf.b.test.data$y))
rf.b.cm

# Model accuracy
rf.b.acc <- rf.b.cm$overall["Accuracy"]
rf.b.acc

# Sensitivity and Specificity
rf.b.sens <- rf.b.cm$byClass["Sensitivity"]
rf.b.sens
rf.b.spec <- rf.b.cm$byClass["Specificity"]
rf.b.spec
```
## RF with rose
## Accuracy - 89.4%, Sensitivity - 98.0%, Specificity - 24.1%
```{r}
set.seed(123)

rf.r.train.data <- RF.rose.train
rf.r.train.data$y <- factor(rf.r.train.data$y)
rf.r.test.data <- RF.rose.test
rf.r.test.data$y <- as.factor(rf.r.test.data$y)

# set cross validation parameters
trControl <- trainControl(method = "cv",
                          number = 10,
                          search = "grid")

# Training using ‘random forest’ algorithm
rf.r.model <- train(y~.,data = rf.r.train.data, method = "rf", 
                    metric = "Accuracy", importance = TRUE,
                    trControl = trControl, ntree = 20)

print(rf.r.model)
plot(rf.r.model)

rf.r.model$bestTune$mtry

# Predict
rf.r.predictions <- rf.r.model %>% predict(rf.r.test.data)

# Error
rf.r.error <- mean(rf.r.test.data$y != rf.r.predictions)
rf.r.error

# Confusion Matrix
rf.r.cm <- caret::confusionMatrix(rf.r.predictions, as.factor(rf.r.test.data$y))
rf.r.cm

# Model accuracy
rf.r.acc <- rf.r.cm$overall["Accuracy"]
rf.r.acc

# Sensitivity and Specificity
rf.r.sens <- rf.r.cm$byClass["Sensitivity"]
rf.r.sens
rf.r.spec <- rf.r.cm$byClass["Specificity"]
rf.r.spec
```

# --------------------------------------------------------

## Ridge with original
## Accuracy - 89.6%, Sensitivity - 98.9%, Specificity - 17.9%
```{r}
set.seed(123)

ridge.o.train <- orig.train
ridge.o.train$y <- as.numeric(as.factor(ridge.o.train$y))
ridge.o.test <- orig.test
ridge.o.test$y <- as.numeric(as.factor(ridge.o.test$y))

ridge.o.model <- linearRidge(y ~ ., data = ridge.o.train)

summary(ridge.o.model)

ridge.o.pred <- round(predict(ridge.o.model, ridge.o.test))  # predict on test data
ridge.o.compare <- cbind (actual=ridge.o.test$y, ridge.o.pred)  # combine

# Error
ridge.o.error <- mean(ridge.o.test$y != ridge.o.pred)
ridge.o.error

# Confusion Matrix
ridge.o.cm <- caret::confusionMatrix(as.factor(ridge.o.pred), as.factor(ridge.o.test$y))
ridge.o.cm

# Model accuracy
ridge.o.acc <- ridge.o.cm$overall["Accuracy"]
ridge.o.acc

# Sensitivity and Specificity
ridge.o.sens <- ridge.o.cm$byClass["Sensitivity"]
ridge.o.sens
ridge.o.spec <- ridge.o.cm$byClass["Specificity"]
ridge.o.spec
```
## Ridge with balanced
## Accuracy - 67.3%, Sensitivity - 68.2%, Specificity - 64.1%
```{r}
set.seed(123)

ridge.b.train <- balanced.train
ridge.b.train$y <- as.numeric(as.factor(ridge.b.train$y))
ridge.b.test <- balanced.test
ridge.b.test$y <- as.numeric(as.factor(ridge.b.test$y))

ridge.b.model <- linearRidge(y ~ ., data = ridge.b.train)

summary(ridge.b.model)

ridge.b.pred <- round(predict(ridge.b.model, ridge.b.test))  # predict on test data
ridge.b.compare <- cbind (actual=ridge.b.test$y, ridge.b.pred)  # combine

# Error
ridge.b.error <- mean(ridge.b.test$y != ridge.b.pred)
ridge.b.error

# Confusion Matrix
ridge.b.cm <- caret::confusionMatrix(as.factor(ridge.b.pred), as.factor(ridge.b.test$y))
ridge.b.cm

# Model accuracy
ridge.b.acc <- ridge.b.cm$overall["Accuracy"]
ridge.b.acc

# Sensitivity and Specificity
ridge.b.sens <- ridge.b.cm$byClass["Sensitivity"]
ridge.b.sens
ridge.b.spec <- ridge.b.cm$byClass["Specificity"]
ridge.b.spec
```
## Ridge with rose
## Accuracy - 67.5%, Sensitivity - 68.2%, Specificity - 61.76%
```{r}
set.seed(123)

ridge.r.train <- rose.train
ridge.r.train$y <- as.numeric(as.factor(ridge.r.train$y))
ridge.r.test <- rose.test
ridge.r.test$y <- as.numeric(as.factor(ridge.r.test$y))

ridge.r.model <- linearRidge(y ~ ., data = ridge.r.train)

summary(ridge.r.model)

ridge.r.pred <- round(predict(ridge.r.model, ridge.r.test))  # predict on test data
ridge.r.compare <- cbind (actual=ridge.r.test$y, ridge.r.pred)  # combine

# Error
ridge.r.error <- mean(ridge.r.test$y != ridge.r.pred)
ridge.r.error

# Confusion Matrix
ridge.r.cm <- caret::confusionMatrix(as.factor(ridge.r.pred), as.factor(ridge.r.test$y))
ridge.r.cm

# Model accuracy
ridge.r.acc <- ridge.r.cm$overall["Accuracy"]
ridge.r.acc

# Sensitivity and Specificity
ridge.r.sens <- ridge.r.cm$byClass["Sensitivity"]
ridge.r.sens
ridge.r.spec <- ridge.r.cm$byClass["Specificity"]
ridge.r.spec
```

# --------------------------------------------------------

## LASSO Regression original
## Accuracy - 89.3%, Sensitivity - 99.0%, Specificity - 14.8%
```{r}
set.seed(123)

lasso.o.train.data <- orig.train
lasso.o.train.data$y <- as.factor(lasso.o.train.data$y)
lasso.o.test.data <- orig.test
lasso.o.test.data$y <- as.factor(lasso.o.test.data$y)

dat.o.train.x <- model.matrix(y~.,lasso.o.train.data)
dat.o.train.y <- lasso.o.train.data[,14]

cvfit <- cv.glmnet(dat.o.train.x, dat.o.train.y, family = "binomial", type.measure = "class", nlambda = 1000)
plot(cvfit)
coef(cvfit, s = "lambda.min")

#CV misclassification error rate 
print("CV Error Rate:")
cvfit$cvm[which(cvfit$lambda==cvfit$lambda.min)]

#Optimal penalty
print("Penalty Value:")
cvfit$lambda.min

#For final model predictions go ahead and refit lasso using entire
lasso.o.model<-glmnet(dat.o.train.x, dat.o.train.y, family = "binomial", type.measure = "class", nlambda = 1001)

dat.o.test.x<-model.matrix(y~.,lasso.o.test.data)
fit.o.pred.lasso <- predict(lasso.o.model, newx = dat.o.test.x, type = "response", s = cvfit$lambda.min, exact = FALSE)
fit.o.pred.lasso <- round(fit.o.pred.lasso)

#Lets use the predicted probablities to classify the observations and make a final confusion matrix for the two models.
cutoff<-0.5
class.o.lasso<-factor(ifelse(fit.o.pred.lasso>cutoff,"2","1"))

# Error
lasso.o.error <- mean(as.numeric(lasso.o.test.data$y) != class.o.lasso)
lasso.o.error

# Confusion Matrix
lasso.o.cm <- caret::confusionMatrix(as.factor(class.o.lasso), as.factor(as.numeric(lasso.o.test.data$y)))
lasso.o.cm

# Model accuracy
lasso.o.acc <- lasso.o.cm$overall["Accuracy"]
lasso.o.acc

# Sensitivity and Specificity
lasso.o.sens <- lasso.o.cm$byClass["Sensitivity"]
lasso.o.sens
lasso.o.spec <- lasso.o.cm$byClass["Specificity"]
lasso.o.spec
```
## LASSO Regression balanced
## Accuracy - 67.9%, Sensitivity - 68.4%, Specificity - 63.4%
```{r}
set.seed(123)

lasso.b.train.data <- balanced.train
lasso.b.train.data$y <- as.factor(lasso.b.train.data$y)
lasso.b.test.data <- balanced.test
lasso.b.test.data$y <- as.factor(lasso.b.test.data$y)

dat.b.train.x <- model.matrix(y~.,lasso.b.train.data)
dat.b.train.y <- lasso.b.train.data[,14]

cvfit <- cv.glmnet(dat.b.train.x, dat.b.train.y, family = "binomial", type.measure = "class", nlambda = 1000)
plot(cvfit)
coef(cvfit, s = "lambda.min")
#CV misclassification error rate 
print("CV Error Rate:")
cvfit$cvm[which(cvfit$lambda==cvfit$lambda.min)]

#Optimal penalty
print("Penalty Value:")
cvfit$lambda.min

#For final model predictions go ahead and refit lasso using entire
lasso.b.model<-glmnet(dat.b.train.x, dat.b.train.y, family = "binomial",lambda=cvfit$lambda.min, type.measure = "class", nlambda = 1001)

dat.b.test.x<-model.matrix(y~.,lasso.b.test.data)
fit.b.pred.lasso <- predict(lasso.b.model, newx = dat.b.test.x, type = "response", s = cvfit$lambda.min)

#Lets use the predicted probablities to classify the observations and make a final confusion matrix for the two models
cutoff<-0.5
class.b.lasso<-factor(ifelse(fit.b.pred.lasso>cutoff,"2","1"))

# Error
lasso.b.error <- mean(as.numeric(lasso.b.test.data$y) != class.b.lasso)
lasso.b.error

# Confusion Matrix
lasso.b.cm <- caret::confusionMatrix(as.factor(class.b.lasso), as.factor(as.numeric(lasso.b.test.data$y)))
lasso.b.cm

# Model accuracy
lasso.b.acc <- lasso.b.cm$overall["Accuracy"]
lasso.b.acc

# Sensitivity and Specificity
lasso.b.sens <- lasso.b.cm$byClass["Sensitivity"]
lasso.b.sens
lasso.b.spec <- lasso.b.cm$byClass["Specificity"]
lasso.b.spec
```
## LASSO Regression Rose
## Accuracy - 89.6%, Sensitivity - 98.4%, Specificity - 20.4%
```{r}
set.seed(123)

lasso.r.train.data <- rose.train
lasso.r.train.data$y <- as.factor(lasso.r.train.data$y)
lasso.r.test.data <- rose.test
lasso.r.test.data$y <- as.factor(lasso.r.test.data$y)

dat.r.train.x <- model.matrix(y~.,lasso.r.train.data)
dat.r.train.y <- lasso.r.train.data[,14]

cvfit <- cv.glmnet(dat.r.train.x, dat.r.train.y, family = "binomial", type.measure = "class", nlambda = 1000)
plot(cvfit)
coef(cvfit, s = "lambda.min")
#CV misclassification error rate is little below .1
print("CV Error Rate:")
cvfit$cvm[which(cvfit$lambda==cvfit$lambda.min)]

#Optimal penalty
print("Penalty Value:")
cvfit$lambda.min

#For final model predictions go ahead and refit lasso using entire
lasso.r.model<-glmnet(dat.r.train.x, dat.r.train.y, family = "binomial",lambda=cvfit$lambda.min)

dat.r.test.x<-model.matrix(y~.,lasso.r.test.data)
fit.r.pred.lasso <- predict(lasso.r.model, newx = dat.r.test.x, type = "response")

#Lets use the predicted probablities to classify the observations and make a final confusion matrix for the two models.  
cutoff<-0.5
class.r.lasso<-factor(ifelse(fit.r.pred.lasso>cutoff,"2","1"))

# Error
lasso.r.error <- mean(as.numeric(lasso.r.test.data$y) != class.r.lasso)
lasso.r.error

# Confusion Matrix
lasso.r.cm <- caret::confusionMatrix(as.factor(class.r.lasso), as.factor(as.numeric(lasso.r.test.data$y)))
lasso.r.cm

# Model accuracy
lasso.r.acc <- lasso.r.cm$overall["Accuracy"]
lasso.r.acc

# Sensitivity and Specificity
lasso.r.sens <- lasso.r.cm$byClass["Sensitivity"]
lasso.r.sens
lasso.r.spec <- lasso.r.cm$byClass["Specificity"]
lasso.r.spec
```

# --------------------------------------------------------

## Net Regression original
## Accuracy - 89.4%, Sensitivity - 98.9%, Specificity - 15.7%
```{r}
set.seed(123)

net.o.train <-orig.train
net.o.train$y <- as.factor(net.o.train$y)
net.o.test <- orig.test
net.o.test$y <- as.factor(net.o.test$y)

net.o.lr <- rbind(net.o.train, net.o.test)
net.o.lr$y <- as.factor(net.o.lr$y)

dummies <- dummyVars(y ~ ., data = net.o.lr)
train_dummies = predict(dummies, newdata = net.o.train)
test_dummies = predict(dummies, newdata = net.o.test)

x = as.matrix(train_dummies)
y_train = net.o.train$y

x_test = as.matrix(test_dummies)
y_test = net.o.test$y

# Set training control
train_cont <- trainControl(method = "repeatedcv",
                           number = 10,
                           repeats = 1,
                           search = "random",
                           verboseIter = TRUE)

# Train the model
elastic_reg <- train(y ~ .,
                     data = net.o.train,
                     method = "glmnet",
                     preProcess = c("center", "scale"),
                     tuneLength = 10,
                     trControl = train_cont)


# Best tuning parameter
elastic_reg$bestTune

# Make predictions on test set
net.o.pred <- predict(elastic_reg, x_test)

# Error
net.o.error <- mean(net.o.test$y != net.o.pred)
net.o.error

# Confusion Matrix
net.o.cm <- caret::confusionMatrix(net.o.pred, as.factor(y_test))
net.o.cm

# Model accuracy
net.o.acc <- net.o.cm$overall["Accuracy"]
net.o.acc

# Sensitivity and Specificity
net.o.sens <- net.o.cm$byClass["Sensitivity"]
net.o.sens
net.o.spec <- net.o.cm$byClass["Specificity"]
net.o.spec
```
## Net Regression balanced
## Accuracy - 68.4%, Sensitivity - 69.1%, Specificity - 63.2%
```{r}
set.seed(123)

net.b.train <-balanced.train
net.b.train$y <- as.factor(net.b.train$y)
net.b.test <- balanced.test
net.b.test$y <- as.factor(net.b.test$y)

net.b.lr <- rbind(net.b.train, net.b.test)
net.b.lr$y <- as.factor(net.b.lr$y)

dummies <- dummyVars(y ~ ., data = net.b.lr)
train_dummies = predict(dummies, newdata = net.b.train)
test_dummies = predict(dummies, newdata = net.b.test)

x = as.matrix(train_dummies)
y_train = net.b.train$y

x_test = as.matrix(test_dummies)
y_test = net.b.test$y

# Set training control
train_cont <- trainControl(method = "repeatedcv",
                           number = 10,
                           repeats = 1,
                           search = "random",
                           verboseIter = TRUE)

# Train the model
elastic_reg <- train(y ~ .,
                     data = net.b.train,
                     method = "glmnet",
                     preProcess = c("center", "scale"),
                     tuneLength = 10,
                     trControl = train_cont)


# Best tuning parameter
elastic_reg$bestTune


# Make predictions on test set
net.b.pred <- predict(elastic_reg, x_test)

# Error
net.b.error <- mean(net.b.test$y != net.b.pred)
net.b.error

# Confusion Matrix
net.b.cm <- caret::confusionMatrix(net.b.pred, as.factor(y_test))
net.b.cm

# Model accuracy
net.b.acc <- net.b.cm$overall["Accuracy"]
net.b.acc

# Sensitivity and Specificity
net.b.sens <- net.b.cm$byClass["Sensitivity"]
net.b.sens
net.b.spec <- net.b.cm$byClass["Specificity"]
net.b.spec
```
## Net Regression rose
## Accuracy - 67.2%, Sensitivity - 67.9%, Specificity - 61.7%
```{r}
set.seed(123)

net.r.train <- rose.train
net.r.train$y <- as.factor(net.r.train$y)
net.r.test <- rose.test
net.r.test$y <- as.factor(net.r.test$y)

net.r.lr <- rbind(net.r.train, net.r.test)
net.r.lr$y <- as.factor(net.r.lr$y)

dummies <- dummyVars(y ~ ., data = net.r.lr)
train_dummies = predict(dummies, newdata = net.r.train)
test_dummies = predict(dummies, newdata = net.r.test)


x = as.matrix(train_dummies)
y_train = net.r.train$y

x_test = as.matrix(test_dummies)
y_test = net.r.test$y

# Set training control
train_cont <- trainControl(method = "repeatedcv",
                           number = 10,
                           repeats = 1,
                           search = "random",
                           verboseIter = TRUE)

# Train the model
elastic_reg <- train(y ~ .,
                     data = net.r.train,
                     method = "glmnet",
                     preProcess = c("center", "scale"),
                     tuneLength = 10,
                     trControl = train_cont)


# Best tuning parameter
elastic_reg$bestTune

# Make predictions on test set
net.r.pred <- predict(elastic_reg, x_test)

# Error
net.r.error <- mean(net.r.test$y != net.r.pred)
net.r.error

# Confusion Matrix
net.r.cm <- caret::confusionMatrix(net.r.pred, as.factor(y_test))
net.r.cm

# Model accuracy
net.r.acc <- net.r.cm$overall["Accuracy"]
net.r.acc

# Sensitivity and Specificity
net.r.sens <- net.r.cm$byClass["Sensitivity"]
net.r.sens
net.r.spec <- net.r.cm$byClass["Specificity"]
net.r.spec
```

# --------------------------------------------------------

## Objective 2 Results LDA/QDA
```{r}
LDA.O.results <- cbind("LDA Original",lda.o.acc, lda.o.sens, lda.o.spec, lda.o.error)
LDA.B.results <- cbind("LDA Balanced",lda.b.acc, lda.b.sens, lda.b.spec, lda.b.error)
LDA.R.results <- cbind("LDA Rose",lda.r.acc, lda.r.sens, lda.r.spec, lda.r.error)
QDA.O.results <- cbind("QDA Original",qda.o.acc, qda.o.sens, qda.o.spec, qda.o.error)
QDA.B.results <- cbind("QDA Balanced",qda.b.acc, qda.b.sens, qda.b.spec, qda.b.error)
QDA.R.results <- cbind("QDA Rose",qda.r.acc, qda.r.sens, qda.r.spec, qda.r.error)


accuracy <- rbind(LDA.O.results, LDA.B.results, LDA.R.results,
                  QDA.O.results, QDA.B.results, QDA.R.results)

colnames(accuracy) <- c("Method", "Accuracy", "Sensitivity", "Specificity", "Error Rate")

accuracytbl <- as.data.frame(accuracy)
accuracytbl$Accuracy <- as.numeric(accuracytbl$Accuracy)
accuracytbl$Sensitivity <- as.numeric(accuracytbl$Sensitivity)
accuracytbl$Specificity <- as.numeric(accuracytbl$Specificity)
accuracytbl$`Error Rate` <- as.numeric(accuracytbl$`Error Rate`)


# Neat table
customGreen = "#009900"
customRed = "#ff7f7f"

options(digits = 3)
as.datatable(formattable(accuracytbl,align = c("l",rep("r", NCOL(accuracytbl) - 1)),
                         list(`Method` = formatter("span", style = ~ style(color = "black", font.weight = "bold")), 
                              'Accuracy' = percent, 
                              'Sensitivity' = percent,
                              'Specificity' = percent,
                              'Error Rate' = percent,
                              'Accuracy' = color_text(customRed, customGreen),
                              'Sensitivity' = color_text(customRed, customGreen),
                              "Specificity" = color_text(customRed, customGreen),
                              'Error Rate' = color_text(customGreen, customRed)))
)
```

## Objective 2 Results All Tables
```{r}
LDA.O.results <- cbind("LDA Original",lda.o.acc, lda.o.sens, lda.o.spec, lda.o.error)
LDA.B.results <- cbind("LDA Balanced",lda.b.acc, lda.b.sens, lda.b.spec, lda.b.error)
LDA.R.results <- cbind("LDA Rose",lda.r.acc, lda.r.sens, lda.r.spec, lda.r.error)
QDA.O.results <- cbind("QDA Original",qda.o.acc, qda.o.sens, qda.o.spec, qda.o.error)
QDA.B.results <- cbind("QDA Balanced",qda.b.acc, qda.b.sens, qda.b.spec, qda.b.error)
QDA.R.results <- cbind("QDA Rose",qda.r.acc, qda.r.sens, qda.r.spec, qda.r.error)
MDA.O.results <- cbind("MDA Original",mda.o.acc, mda.o.sens, mda.o.spec, mda.o.error)
MDA.B.results <- cbind("MDA Balanced",mda.b.acc, mda.b.sens, mda.b.spec, mda.b.error)
MDA.R.results <- cbind("MDA Rose",mda.r.acc, mda.r.sens, mda.r.spec, mda.r.error)
FDA.O.results <- cbind("FDA Original",fda.o.acc, fda.o.sens, fda.o.spec, fda.o.error)
FDA.B.results <- cbind("FDA Balanced",fda.b.acc, fda.b.sens, fda.b.spec, fdab.error)
FDA.R.results <- cbind("FDA Rose",fda.r.acc, fda.r.sens, fda.r.spec, fda.r.error)
RIDGE.O.results <- cbind("Ridge Original",ridge.o.acc, ridge.o.sens, ridge.o.spec, ridge.o.error)
RIDGE.B.results <- cbind("Ridge Balanced",ridge.b.acc, ridge.b.sens, ridge.b.spec, ridge.b.error)
RIDGE.R.results <- cbind("Ridge Rose",ridge.r.acc, ridge.r.sens, ridge.r.spec, ridge.r.error)
LASSO.O.results <- cbind("Lasso Original",lasso.o.acc, lasso.o.sens, lasso.o.spec, lasso.o.error)
LASSO.B.results <- cbind("Lasso Balanced",lasso.b.acc, lasso.b.sens, lasso.b.spec, lasso.b.error)
LASSO.R.results <- cbind("Lasso Rose",lasso.r.acc, lasso.r.sens, lasso.r.spec, lasso.r.error)
NET.O.results <- cbind("Net Original",net.o.acc, net.o.sens, net.o.spec, net.o.error)
NET.B.results <- cbind("Net Balanced",net.b.acc, net.b.sens, net.b.spec, net.b.error)
NET.R.results <- cbind("Net Rose",net.r.acc, net.r.sens, net.r.spec, net.r.error)
RF.O.results <- cbind("RF Original",rf.o.acc, rf.o.sens, rf.o.spec, rf.o.error)
RF.B.results <- cbind("RF Balanced",rf.b.acc, rf.b.sens, rf.b.spec, rf.b.error)
RF.R.results <- cbind("RF Rose",rf.r.acc, rf.r.sens, rf.r.spec, rf.r.error)

accuracy <- rbind(LDA.O.results, LDA.B.results, LDA.R.results,
                  QDA.O.results, QDA.B.results, QDA.R.results,
                  MDA.O.results, MDA.B.results, MDA.R.results,
                  FDA.O.results, FDA.B.results, FDA.R.results,
                  RIDGE.O.results, RIDGE.B.results, RIDGE.R.results,
                  LASSO.O.results, LASSO.B.results, LASSO.R.results,
                  NET.O.results, NET.B.results, NET.R.results,
                  RF.O.results, RF.B.results, RF.R.results)

colnames(accuracy) <- c("Method", "Accuracy", "Sensitivity", "Specificity", "ErrorRate")

accuracytbl <- as.data.frame(accuracy)
accuracytbl$Accuracy <- as.numeric(accuracytbl$Accuracy)
accuracytbl$Sensitivity <- as.numeric(accuracytbl$Sensitivity)
accuracytbl$Specificity <- as.numeric(accuracytbl$Specificity)
accuracytbl$ErrorRate <- as.numeric(accuracytbl$ErrorRate)

# Neat table
customGreen = "#009900"
customRed = "#ff7f7f"

as.datatable(formattable(accuracytbl, digits = 3,
                         align = c("l",rep("r", NCOL(accuracytbl) - 1)),
                         list(`Method` = formatter("span", style = ~ style(color = "black", font.weight = "bold")), 
                              'Accuracy' = percent, 
                              'Sensitivity' = percent,
                              'Specificity' = percent,
                              'Accuracy' = color_text(customRed, customGreen),
                              'Sensitivity' = color_text(customRed, customGreen),
                              "Specificity" = color_text(customRed, customGreen),
                              "ErrorRate" = color_text(customGreen, customRed)))
)
```






#EXTRA MODELS DEEMED UNNECESSARY
## RDA with original
## Accuracy - 89.6%, Sensitivity - 98.6%, Specificity - 20.1%
```{r}
set.seed(123)

# Fit the model
rda.o.model <- rda(y ~ ., data = orig.train)

# Make predictions
rda.o.predictions <- rda.o.model %>% predict(orig.test)

# Error
rda.o.error <- mean(orig.test$y != rda.o.predictions$class)
rda.o.error

# Confusion Matrix
rda.o.cm <- caret::confusionMatrix(rda.o.predictions$class, as.factor(orig.test$y))
rda.o.cm

# Model accuracy
rda.o.acc <- rda.o.cm$overall["Accuracy"]
rda.o.acc

# Sensitivity and Specificity
rda.o.sens <- rda.o.cm$byClass["Sensitivity"]
rda.o.sens
rda.o.spec <- rda.o.cm$byClass["Specificity"]
rda.o.spec
```
## RDA with balanced
## Accuracy - 82.2%, Sensitivity - 86.1%, Specificity - 51.9%
```{r}
set.seed(123)

# Fit the model
rda.b.model <- rda(y ~ ., data = balanced.train)

# Make predictions
rda.b.predictions <- rda.b.model %>% predict(balanced.test)

# Error
rda.b.error <- mean(balanced.test$y != rda.b.predictions$class)
rda.b.error

# Confusion Matrix
rda.b.cm <- caret::confusionMatrix(rda.b.predictions$class, as.factor(balanced.test$y))
rda.b.cm

# Model accuracy
rda.b.acc <- rda.b.cm$overall["Accuracy"]
rda.b.acc

# Sensitivity and Specificity
rda.b.sens <- rda.b.cm$byClass["Sensitivity"]
rda.b.sens
rda.b.spec <- rda.b.cm$byClass["Specificity"]
rda.b.spec
```
## RDA with rose
## Accuracy - 87.7%, Sensitivity - 94.6%, Specificity - 35.4%
```{r}
set.seed(123)

# Fit the model
rda.r.model <- rda(y ~ ., data = rose.train)

# Make predictions
rda.r.predictions <- rda.r.model %>% predict(rose.test)

# Error
rda.r.error <- mean(rose.test$y != rda.r.predictions$class)
rda.r.error

# Confusion Matrix
rda.r.cm <- caret::confusionMatrix(rda.r.predictions$class, as.factor(rose.test$y))
rda.r.cm

# Model accuracy
rda.r.acc <- rda.r.cm$overall["Accuracy"]
rda.r.acc

# Sensitivity and Specificity
rda.r.sens <- rda.r.cm$byClass["Sensitivity"]
rda.r.sens
rda.r.spec <- rda.r.cm$byClass["Specificity"]
rda.r.spec
```
